# LLM Router Configuration
# Environment variables take precedence over values in this file
# Format: LLM_<PROVIDER_NAME>_<SETTING>

# Global settings
loadBalancingStrategy: round_robin  # Use round_robin since we only have one provider
defaultModel: mistral:latest  # Use mistral:latest as default since it's available in Ollama

# Provider configurations
providers:
  # OpenAI Provider - DISABLED (no API key)
  - name: openai-prod
    type: openai
    enabled: false  # Disabled because no API key is configured
    priority: 1  # Higher number = higher priority
    apiKey: ${OPENAI_API_KEY}  # Can be overridden by LLM_OPENAI_PROD_API_KEY
    baseUrl: https://api.openai.com/v1
    models:
      - name: gpt-3.5-turbo
        costPer1kInputTokens: 0.0015
        costPer1kOutputTokens: 0.002
        maxTokens: 4096
      - name: gpt-4
        costPer1kInputTokens: 0.03
        costPer1kOutputTokens: 0.06
        maxTokens: 8192
    
    # Circuit breaker configuration
    circuitBreaker:
      failureThreshold: 5    # Number of failures before opening the circuit
      successThreshold: 3    # Number of successful attempts to close the circuit
      timeout: 60000         # Time in ms to wait before attempting to close the circuit
    
    # Retry policy
    retry:
      maxAttempts: 3         # Maximum number of retry attempts
      initialDelay: 1000     # Initial delay in ms
      maxDelay: 30000        # Maximum delay in ms
      factor: 2              # Exponential backoff factor

  # Anthropic Provider - DISABLED (no API key)
  - name: anthropic-prod
    type: anthropic
    enabled: false  # Disabled because no API key is configured
    priority: 2
    apiKey: ${ANTHROPIC_API_KEY}
    baseUrl: https://api.anthropic.com/v1
    models:
      - name: claude-2
        costPer1kInputTokens: 0.01102
        costPer1kOutputTokens: 0.03268
        maxTokens: 100000
    circuitBreaker:
      failureThreshold: 5
      successThreshold: 3
      timeout: 60000
    retry:
      maxAttempts: 3
      initialDelay: 1000
      maxDelay: 30000
      factor: 2

  # Google Gemini Provider - DISABLED (no API key)
  - name: google-prod
    type: google
    enabled: false  # Disabled because no API key is configured
    priority: 2
    apiKey: ${GOOGLE_API_KEY}
    baseUrl: https://generativelanguage.googleapis.com/v1beta
    models:
      - name: gemini-pro
        costPer1kInputTokens: 0.00025
        costPer1kOutputTokens: 0.0005
        maxTokens: 30720
    circuitBreaker:
      failureThreshold: 5
      successThreshold: 3
      timeout: 60000
    retry:
      maxAttempts: 3
      initialDelay: 1000
      maxDelay: 30000
      factor: 2

  # Ollama Provider (local) - ENABLED as fallback
  - name: ollama-local
    type: ollama
    enabled: true
    priority: 1  # Highest priority since it's the only available provider
    baseUrl: http://localhost:11434
    models:
      - name: mistral:latest  # Use mistral:latest as primary model
        costPer1kInputTokens: 0
        costPer1kOutputTokens: 0
        maxTokens: 4096
      - name: llama2  # Keep llama2 as fallback
        costPer1kInputTokens: 0
        costPer1kOutputTokens: 0
        maxTokens: 4096
    circuitBreaker:
      failureThreshold: 3
      successThreshold: 2
      timeout: 30000
    retry:
      maxAttempts: 2
      initialDelay: 500
      maxDelay: 10000
      factor: 1.5
